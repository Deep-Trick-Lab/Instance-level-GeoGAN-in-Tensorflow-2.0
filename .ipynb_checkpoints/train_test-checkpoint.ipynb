{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, 'Not enough GPU hardware devices available'\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from STNFunction import *\n",
    "from losses import *\n",
    "from Generator import *\n",
    "from Discriminator import *\n",
    "from Dataset import *\n",
    "from utils.image_landmark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from import get_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8 \n",
    "EPOCHS = 100\n",
    "NUMIMAGES = 1000\n",
    "BUFFER_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_cls = 0.5\n",
    "lambda_flow = 0.1\n",
    "lambda_mask = 0.1\n",
    "lambda_landmark = 10\n",
    "lambda_reco = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (280,140,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model(IMG_SIZE,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = AttrbuteMultiscalePatchDisc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_loss_func = flow_loss()\n",
    "lsgan_loss_func = lsgan_loss()\n",
    "g_LrDecay = tf.keras.optimizers.schedules.ExponentialDecay(0.002,decay_steps=3000,\n",
    "                                                          decay_rate=0.95,\n",
    "                                                          staircase=True)\n",
    "d_LrDecay = tf.keras.optimizers.schedules.ExponentialDecay(0.002,decay_steps=1800,\n",
    "                                                          decay_rate=0.95,\n",
    "                                                          staircase=True)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.002)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(d_LrDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(Ax,By,landmark_Ax,landmark_By,epoch):\n",
    "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "        g_items = generator(Ax,By,epoch,True)\n",
    "        # 1. TV_reg loss\n",
    "        G_flow_loss = 0.0\n",
    "        for flow in g_items['flows']:\n",
    "            G_flow_loss += flow_loss_func.totalVariation_loss(flow)\n",
    "        G_flow_loss = lambda_flow * tf.cast(G_flow_loss,'float32')\n",
    "        # 2. landmark loss (By_flow)\n",
    "        G_land_loss = flow_loss_func.landmark_loss(landmark_By,landmark_Ax,g_items['flows'][0])\n",
    "        G_land_loss += flow_loss_func.landmark_loss(landmark_Ax,landmark_By,g_items['flows'][1])\n",
    "        G_land_loss = lambda_landmark * tf.cast(G_land_loss,'float32')\n",
    "        # 3. recon loss\n",
    "            # 3-1. mask\n",
    "        G_mask_loss = 0.0\n",
    "        for mask in g_items['masks']:\n",
    "            G_mask_loss += recon_loss(mask,tf.zeros_like(mask))\n",
    "        G_mask_loss = lambda_mask * tf.cast(G_mask_loss,'float32')\n",
    "            # 3-2. add-removal recon\n",
    "        G_rcon_loss = recon_loss(By,g_items['fakeBx_to_By']) + recon_loss(Ax,g_items['fakeAy_to_Ax'])\n",
    "        G_rcon_loss = lambda_reco * tf.cast(G_rcon_loss,'float32')\n",
    "        #-------------------------------------------\n",
    "        \n",
    "        # 4. GAN loss\n",
    "        D_fake_loss = 0.0\n",
    "        D_real_loss = 0.0\n",
    "        GAN_loss = 0.0\n",
    "        G_cls_loss = 0.0\n",
    "        D_cls_loss = 0.0\n",
    "        # -- For Generator\n",
    "        for itemname in ['fake_Ay','fakeAy_to_Ax','fakeBx_to_By','fake_Bx']:\n",
    "            item = g_items[itemname]\n",
    "            _, pred, attr_pred = discriminator(item)\n",
    "            for pred_i in pred:\n",
    "                D_fake_loss += lsgan_loss_func.loss_func(pred_i, False)\n",
    "                GAN_loss += lsgan_loss_func.loss_func(pred_i, True)\n",
    "            \n",
    "            for attr in attr_pred:\n",
    "                if itemname in 'fake_Ay' or itemname in 'By':\n",
    "                    attr_label = tf.ones_like(attr)\n",
    "                else:\n",
    "                    attr_label = tf.zeros_like(attr)\n",
    "                G_cls_loss += lambda_cls * tf.cast(cls_loss(attr,attr_label),'float32')\n",
    "        D_fake_loss = 0.5 * D_fake_loss\n",
    "        # -- For Real Data\n",
    "        for (img,hasAttr) in [(Ax,False),(By,True)]:\n",
    "            _, pred, attr_pred = discriminator(img)\n",
    "            for pred_i in pred:\n",
    "                D_real_loss += lsgan_loss_func.loss_func(pred_i, True)\n",
    "            for attr in attr_pred:\n",
    "                if hasAttr:\n",
    "                    label = tf.ones_like(attr)\n",
    "                else:\n",
    "                    label = tf.zeros_like(attr)\n",
    "                D_cls_loss += lambda_cls * tf.cast(cls_loss(attr,label),'float32')\n",
    "        # -- CAST\n",
    "        D_fake_loss = tf.cast(D_fake_loss,'float32')\n",
    "        D_real_loss = tf.cast(D_real_loss,'float32')\n",
    "        GAN_loss = tf.cast(GAN_loss,'float32')\n",
    "        \n",
    "        DIS_loss = D_fake_loss+D_real_loss\n",
    "        flow_loss = G_flow_loss+G_land_loss\n",
    "        \n",
    "        G_loss = G_rcon_loss+G_cls_loss+GAN_loss+flow_loss\n",
    "        D_loss = DIS_loss+D_cls_loss\n",
    "    gradients_of_generator = g_tape.gradient(G_loss,generator.trainable_variables)\n",
    "    \n",
    "    gradients_of_discriminator = d_tape.gradient(D_loss,discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,discriminator.trainable_variables))\n",
    "    return_items = {}\n",
    "    return_items[\"G_flow_loss\"] = G_flow_loss\n",
    "    return_items[\"G_land_loss\"] = G_land_loss\n",
    "    return_items[\"G_mask_loss\"] = G_mask_loss\n",
    "    return_items[\"G_rcon_loss\"] = G_rcon_loss\n",
    "    return_items[\"G_cls_loss\"] = G_cls_loss\n",
    "    return_items[\"GAN_loss\"] = GAN_loss\n",
    "    return_items[\"D_fake_loss\"] = D_fake_loss\n",
    "    return_items[\"D_real_loss\"] = D_real_loss\n",
    "    return_items[\"D_cls_loss\"] = D_cls_loss\n",
    "    return_items[\"G_loss\"] = G_loss\n",
    "    return_items[\"D_loss\"] = D_loss\n",
    "    return return_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- for whole train step\n",
    "lossnames = [\"G_flow_loss\",\"G_land_loss\",\"G_mask_loss\",\"G_rcon_loss\",\"G_cls_loss\",\"GAN_loss\",\n",
    "             \"D_fake_loss\",\"D_real_loss\",\"D_cls_loss\",\"G_loss\",\"D_loss\"]\n",
    "metrics_list = []\n",
    "for itemname in lossnames:\n",
    "    metrics_list.append(tf.keras.metrics.Mean(itemname, dtype=tf.float32))\n",
    "\n",
    "for epoch in range(1,200+1):\n",
    "    for (one_Ax, one_Ax_landmark), (one_By, one_By_landmark) in zip(Ax_ds, By_ds):\n",
    "        # train_flownet_step(one_Ax,one_By,one_Ax_landmark,one_By_landmark,tf.cast(epoch,'float32'))\n",
    "        train_items = train_step(one_Ax,one_By,one_Ax_landmark,one_By_landmark,tf.cast(epoch,'float32'))\n",
    "        for (idx, itemname) in enumerate(lossnames):\n",
    "            metrics_list[idx](train_items[itemname])\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    for idx,itemname in enumerate(lossnames):\n",
    "        print(\"    {}: {:.4f}\".format(itemname,metrics_list[idx].result()))\n",
    "    #print(\"epoch: {}, G_loss: {:.4f}, D_loss: {:.4f}, flow_loss: {:.3}\".format(epoch, \n",
    "    #                                                         metrics_list[9].result(),metrics_list[10].result(),\n",
    "    #                                                         metrics_list[0].result()+metrics_list[1].result()))\n",
    "    for metric in metrics_list:\n",
    "        metric.reset_states()\n",
    "    if epoch % 5 == 0:\n",
    "        for (one_Ax, one_Ax_landmark), (one_By, one_By_landmark) in zip(Ax_ds, By_ds):\n",
    "            g_items = generator(one_Ax,one_By,tf.cast(epoch,'float32'),False)\n",
    "            save_images([one_Ax,one_By,g_items['By_warpped'].numpy(),\n",
    "                         g_items['masks'][2][:,:,:,0],\n",
    "                         g_items['masks'][0][:,:,:,0],\n",
    "                         g_items['raw_fake_Ay'].numpy(),g_items['fake_Ay'].numpy(),g_items['residual_Ay'].numpy(),\n",
    "                         g_items['fakeAy_to_Ax'].numpy(),g_items['masks'][1][:,:,:,0],\n",
    "                         g_items['fake_Bx'].numpy(),g_items['fakeBx_to_By'].numpy()],\n",
    "                        epoch,BATCH_SIZE,'GeoGAN1126')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Just for flow net\n",
    "lossnames = [\"G_flow_loss\",\"G_land_loss\"]\n",
    "metrics_list = []\n",
    "for itemname in lossnames:\n",
    "    metrics_list.append(tf.keras.metrics.Mean(itemname, dtype=tf.float32))\n",
    "    \n",
    "for epoch in range(1,400+1):\n",
    "    for (one_Ax, one_Ax_landmark), (one_By, one_By_landmark) in zip(Ax_ds, By_ds):\n",
    "        train_items = train_flowNet_step(one_Ax,one_By,one_Ax_landmark,one_By_landmark,tf.cast(epoch,'float32'))\n",
    "        for (idx, itemname) in enumerate(lossnames):\n",
    "            metrics_list[idx](train_items[itemname])\n",
    "    print(\"epoch: {}, G_flow_loss: {}, G_land_loss: {}\".format(epoch, metrics_list[0].result(),metrics_list[1].result()))\n",
    "    metrics_list[0].reset_states()\n",
    "    metrics_list[1].reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- show temp image(processing)\n",
    "fig = plt.figure(figsize=(75, 75))\n",
    "gs = matplotlib.gridspec.GridSpec(1, 4, width_ratios=[1, 1, 1, 1],\n",
    "        wspace=0.0, hspace=0.0, top=0.95, bottom=0.05, left=0.1, right=0.2)\n",
    "img_idx = 6\n",
    "imgs = [testAx[img_idx],testBy[img_idx],g_items['fakeBx_to_By'][img_idx],g_items['masks'][1][img_idx,:,:,0]]\n",
    "for idx,img in enumerate(imgs):\n",
    "    ax = plt.subplot(gs[0,idx])\n",
    "    ax.axis('off')\n",
    "    ax.imshow(np.clip((img+1)/2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- show temp image(showing)\n",
    "fig = plt.figure(figsize=(75, 75))\n",
    "gs = matplotlib.gridspec.GridSpec(1, 4, width_ratios=[1, 1, 1, 1],\n",
    "        wspace=0.0, hspace=0.0, top=0.95, bottom=0.05, left=0.1, right=0.2)\n",
    "img_idx = 7\n",
    "imgs = [testAx[img_idx],testBy[img_idx],test_By_warpped[img_idx],test_raw_Ay[img_idx]]\n",
    "for idx,img in enumerate(imgs):\n",
    "    ax = plt.subplot(gs[0,idx])\n",
    "    ax.axis('off')\n",
    "    ax.imshow(np.clip((img+1)/2,0,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
