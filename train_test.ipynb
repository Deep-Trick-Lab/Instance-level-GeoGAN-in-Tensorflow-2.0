{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, 'Not enough GPU hardware devices available'\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from STNFunction import *\n",
    "from losses import *\n",
    "from Generator import *\n",
    "from Discriminator import *\n",
    "from utils.image_landmark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.coco2017_load import get_files_and_landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8 \n",
    "EPOCHS = 100\n",
    "NUMIMAGES = 1000\n",
    "BUFFER_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_cls = 0.5\n",
    "lambda_flow = 0.1\n",
    "lambda_mask = 0.1\n",
    "lambda_landmark = 10\n",
    "lambda_reco = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "import os.path\n",
    "datasetPath = pathlib.Path(\"../Dataset/coco2017\")\n",
    "filenames, landmarks = get_files_and_landmarks_list(datasetPath/\"annotations\"/\"skeleton_train2017_related.json\",\\\n",
    "                                                datasetPath/\"skeleton_train2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (280,140,3)\n",
    "def map_coco2017_skeleton(filename,landmark):\n",
    "    img = tf.io.read_file(tf.convert_to_tensor(filename))\n",
    "    img = tf.io.decode_jpeg(img)\n",
    "    img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "    img = tf.image.resize_with_pad(img,IMG_SIZE[0],IMG_SIZE[1],method=tf.image.ResizeMethod.BILINEAR,antialias=False)\n",
    "    # landmark = landmark_resize_with_pad(landmark,img.shape[0],img.shape[1],IMG_SIZE[0],IMG_SIZE[1])\n",
    "    [newlandmark] = tf.py_function(func=landmark_resize_with_pad,\n",
    "                                       inp=[landmark,img,IMG_SIZE[0],IMG_SIZE[1]],\n",
    "                                       Tout=[tf.float32])\n",
    "    # newlandmark = landmark\n",
    "    return img,newlandmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((filenames, landmarks))\n",
    "ds = ds.map(map_coco2017_skeleton) # ,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "Ax_ds = ds.shard(2,0).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "By_ds = ds.shard(2,1).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.26617125, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1.7928572  0.17857143]\n",
      "  [1.8785714  0.15      ]\n",
      "  [1.7428571  0.12857144]\n",
      "  [1.9285715  0.175     ]\n",
      "  [1.6071428  0.13928571]\n",
      "  [2.0285714  0.32142857]\n",
      "  [1.3214285  0.27142859]\n",
      "  [2.2714286  0.35714287]\n",
      "  [0.98571426 0.42142856]\n",
      "  [2.3214285  0.38214287]\n",
      "  [0.87142855 0.6142857 ]\n",
      "  [1.3642857  0.7214286 ]\n",
      "  [0.94285715 0.65      ]\n",
      "  [1.3142858  0.825     ]\n",
      "  [0.67142856 0.78571427]\n",
      "  [0.9357143  1.2714286 ]\n",
      "  [0.3857143  1.1714286 ]]\n",
      "\n",
      " [[0.1        0.02857143]\n",
      "  [0.10714286 0.025     ]\n",
      "  [0.08571429 0.025     ]\n",
      "  [0.12857144 0.025     ]\n",
      "  [0.07857143 0.025     ]\n",
      "  [0.15       0.05357143]\n",
      "  [0.05714286 0.05357143]\n",
      "  [0.19285715 0.09642857]\n",
      "  [0.02142857 0.09285714]\n",
      "  [0.14285715 0.12142857]\n",
      "  [0.02857143 0.11785714]\n",
      "  [0.12142857 0.13214286]\n",
      "  [0.07142857 0.13214286]\n",
      "  [0.12857144 0.19642857]\n",
      "  [0.06428572 0.19642857]\n",
      "  [0.13571429 0.24642856]\n",
      "  [0.07142857 0.24642856]]\n",
      "\n",
      " [[0.15       0.03214286]\n",
      "  [0.16428572 0.02857143]\n",
      "  [0.13571429 0.02857143]\n",
      "  [0.19285715 0.03214286]\n",
      "  [0.11428571 0.03571429]\n",
      "  [0.22857143 0.08571429]\n",
      "  [0.07142857 0.08571429]\n",
      "  [0.25714287 0.15357143]\n",
      "  [0.05       0.15      ]\n",
      "  [0.3        0.20714286]\n",
      "  [0.07857143 0.14642857]\n",
      "  [0.21428572 0.18928571]\n",
      "  [0.10714286 0.18928571]\n",
      "  [0.17142858 0.25      ]\n",
      "  [0.12857144 0.28928572]\n",
      "  [0.16428572 0.38214287]\n",
      "  [0.15       0.38214287]]\n",
      "\n",
      " [[0.80714285 0.06071429]\n",
      "  [0.8428571  0.05357143]\n",
      "  [0.8        0.04285714]\n",
      "  [0.85714287 0.07857143]\n",
      "  [0.75       0.05357143]\n",
      "  [0.87142855 0.14642857]\n",
      "  [0.6785714  0.125     ]\n",
      "  [0.95714283 0.22857143]\n",
      "  [0.6357143  0.20357142]\n",
      "  [0.79285717 0.19642857]\n",
      "  [0.53571427 0.23214285]\n",
      "  [0.7285714  0.3392857 ]\n",
      "  [0.5714286  0.325     ]\n",
      "  [0.67142856 0.49642858]\n",
      "  [0.33571428 0.42142856]\n",
      "  [0.85714287 0.5928571 ]\n",
      "  [0.15       0.5642857 ]]\n",
      "\n",
      " [[0.29285714 0.07142857]\n",
      "  [0.31428573 0.06071429]\n",
      "  [0.2642857  0.06071429]\n",
      "  [0.35       0.06785715]\n",
      "  [0.22142857 0.06071429]\n",
      "  [0.37857142 0.13571429]\n",
      "  [0.15714286 0.12857144]\n",
      "  [0.42142856 0.20714286]\n",
      "  [0.07857143 0.18571429]\n",
      "  [0.40714285 0.2607143 ]\n",
      "  [0.05       0.24285714]\n",
      "  [0.32857144 0.3       ]\n",
      "  [0.17142858 0.29642856]\n",
      "  [0.32857144 0.3642857 ]\n",
      "  [0.14285715 0.3607143 ]\n",
      "  [0.32857144 0.43928573]\n",
      "  [0.07142857 0.43571427]]\n",
      "\n",
      " [[0.2642857  0.08928572]\n",
      "  [0.2857143  0.06428572]\n",
      "  [0.22857143 0.07142857]\n",
      "  [0.35       0.07142857]\n",
      "  [0.20714286 0.08214286]\n",
      "  [0.40714285 0.15714286]\n",
      "  [0.12857144 0.15714286]\n",
      "  [0.45       0.24642856]\n",
      "  [0.05714286 0.25357142]\n",
      "  [0.43571427 0.3       ]\n",
      "  [0.07142857 0.31428573]\n",
      "  [0.32857144 0.34285715]\n",
      "  [0.15714286 0.3392857 ]\n",
      "  [0.3642857  0.5214286 ]\n",
      "  [0.10714286 0.4107143 ]\n",
      "  [0.34285715 0.6785714 ]\n",
      "  [0.15       0.5285714 ]]\n",
      "\n",
      " [[0.67142856 0.13214286]\n",
      "  [0.73571426 0.1       ]\n",
      "  [0.60714287 0.09642857]\n",
      "  [0.80714285 0.13214286]\n",
      "  [0.5285714  0.125     ]\n",
      "  [0.8428571  0.29285714]\n",
      "  [0.41428572 0.2857143 ]\n",
      "  [1.0357143  0.46071428]\n",
      "  [0.11428571 0.46071428]\n",
      "  [0.80714285 0.5535714 ]\n",
      "  [0.32857144 0.58214283]\n",
      "  [0.85       0.67142856]\n",
      "  [0.5857143  0.7285714 ]\n",
      "  [0.92142856 1.0035714 ]\n",
      "  [0.8357143  1.025     ]\n",
      "  [0.9142857  1.2571429 ]\n",
      "  [1.0428572  1.2678572 ]]\n",
      "\n",
      " [[0.17142858 0.04285714]\n",
      "  [0.18571429 0.03571429]\n",
      "  [0.14285715 0.03571429]\n",
      "  [0.21428572 0.04285714]\n",
      "  [0.11428571 0.04642857]\n",
      "  [0.27142859 0.11785714]\n",
      "  [0.09285714 0.12142857]\n",
      "  [0.32142857 0.19642857]\n",
      "  [0.03571429 0.20357142]\n",
      "  [0.32857144 0.2642857 ]\n",
      "  [0.01428571 0.27857143]\n",
      "  [0.24285714 0.27142859]\n",
      "  [0.12142857 0.26785713]\n",
      "  [0.24285714 0.38214287]\n",
      "  [0.10714286 0.3857143 ]\n",
      "  [0.23571429 0.49285713]\n",
      "  [0.10714286 0.49285713]]], shape=(8, 17, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for (a,b) in Ax_ds:\n",
    "    print(tf.reduce_mean(a))\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model(IMG_SIZE,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = AttrbuteMultiscalePatchDisc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_loss_func = flow_loss()\n",
    "lsgan_loss_func = lsgan_loss()\n",
    "g_LrDecay = tf.keras.optimizers.schedules.ExponentialDecay(0.002,decay_steps=3000,\n",
    "                                                          decay_rate=0.95,\n",
    "                                                          staircase=True)\n",
    "d_LrDecay = tf.keras.optimizers.schedules.ExponentialDecay(0.002,decay_steps=1800,\n",
    "                                                          decay_rate=0.95,\n",
    "                                                          staircase=True)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.002)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(d_LrDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(Ax,By,landmark_Ax,landmark_By,epoch):\n",
    "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "        g_items = generator(Ax,By,epoch,True)\n",
    "        # 1. TV_reg loss\n",
    "        G_flow_loss = 0.0\n",
    "        for flow in g_items['flows']:\n",
    "            G_flow_loss += flow_loss_func.totalVariation_loss(flow)\n",
    "        G_flow_loss = lambda_flow * tf.cast(G_flow_loss,'float32')\n",
    "        # 2. landmark loss (By_flow)\n",
    "        G_land_loss = flow_loss_func.landmark_loss(landmark_By,landmark_Ax,g_items['flows'][0])\n",
    "        G_land_loss += flow_loss_func.landmark_loss(landmark_Ax,landmark_By,g_items['flows'][1])\n",
    "        G_land_loss = lambda_landmark * tf.cast(G_land_loss,'float32')\n",
    "        # 3. recon loss\n",
    "            # 3-1. mask\n",
    "        G_mask_loss = 0.0\n",
    "        for mask in g_items['masks']:\n",
    "            G_mask_loss += recon_loss(mask,tf.zeros_like(mask))\n",
    "        G_mask_loss = lambda_mask * tf.cast(G_mask_loss,'float32')\n",
    "            # 3-2. add-removal recon\n",
    "        G_rcon_loss = recon_loss(By,g_items['fakeBx_to_By']) + recon_loss(Ax,g_items['fakeAy_to_Ax'])\n",
    "        G_rcon_loss = lambda_reco * tf.cast(G_rcon_loss,'float32')\n",
    "        #-------------------------------------------\n",
    "        \n",
    "        # 4. GAN loss\n",
    "        D_fake_loss = 0.0\n",
    "        D_real_loss = 0.0\n",
    "        GAN_loss = 0.0\n",
    "        G_cls_loss = 0.0\n",
    "        D_cls_loss = 0.0\n",
    "        # -- For Generator\n",
    "        for itemname in ['fake_Ay','fakeAy_to_Ax','fakeBx_to_By','fake_Bx']:\n",
    "            item = g_items[itemname]\n",
    "            _, pred, attr_pred = discriminator(item)\n",
    "            for pred_i in pred:\n",
    "                D_fake_loss += lsgan_loss_func.loss_func(pred_i, False)\n",
    "                GAN_loss += lsgan_loss_func.loss_func(pred_i, True)\n",
    "            \n",
    "            for attr in attr_pred:\n",
    "                if itemname in 'fake_Ay' or itemname in 'By':\n",
    "                    attr_label = tf.ones_like(attr)\n",
    "                else:\n",
    "                    attr_label = tf.zeros_like(attr)\n",
    "                G_cls_loss += lambda_cls * tf.cast(cls_loss(attr,attr_label),'float32')\n",
    "        D_fake_loss = 0.5 * D_fake_loss\n",
    "        # -- For Real Data\n",
    "        for (img,hasAttr) in [(Ax,False),(By,True)]:\n",
    "            _, pred, attr_pred = discriminator(img)\n",
    "            for pred_i in pred:\n",
    "                D_real_loss += lsgan_loss_func.loss_func(pred_i, True)\n",
    "            for attr in attr_pred:\n",
    "                if hasAttr:\n",
    "                    label = tf.ones_like(attr)\n",
    "                else:\n",
    "                    label = tf.zeros_like(attr)\n",
    "                D_cls_loss += lambda_cls * tf.cast(cls_loss(attr,label),'float32')\n",
    "        # -- CAST\n",
    "        D_fake_loss = tf.cast(D_fake_loss,'float32')\n",
    "        D_real_loss = tf.cast(D_real_loss,'float32')\n",
    "        GAN_loss = tf.cast(GAN_loss,'float32')\n",
    "        \n",
    "        DIS_loss = D_fake_loss+D_real_loss\n",
    "        flow_loss = G_flow_loss+G_land_loss\n",
    "        \n",
    "        G_loss = G_rcon_loss+G_cls_loss+GAN_loss+flow_loss\n",
    "        D_loss = DIS_loss+D_cls_loss\n",
    "    gradients_of_generator = g_tape.gradient(G_loss,generator.trainable_variables)\n",
    "    \n",
    "    gradients_of_discriminator = d_tape.gradient(D_loss,discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,discriminator.trainable_variables))\n",
    "    return_items = {}\n",
    "    return_items[\"G_flow_loss\"] = G_flow_loss\n",
    "    return_items[\"G_land_loss\"] = G_land_loss\n",
    "    return_items[\"G_mask_loss\"] = G_mask_loss\n",
    "    return_items[\"G_rcon_loss\"] = G_rcon_loss\n",
    "    return_items[\"G_cls_loss\"] = G_cls_loss\n",
    "    return_items[\"GAN_loss\"] = GAN_loss\n",
    "    return_items[\"D_fake_loss\"] = D_fake_loss\n",
    "    return_items[\"D_real_loss\"] = D_real_loss\n",
    "    return_items[\"D_cls_loss\"] = D_cls_loss\n",
    "    return_items[\"G_loss\"] = G_loss\n",
    "    return_items[\"D_loss\"] = D_loss\n",
    "    return return_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- for whole train step\n",
    "lossnames = [\"G_flow_loss\",\"G_land_loss\",\"G_mask_loss\",\"G_rcon_loss\",\"G_cls_loss\",\"GAN_loss\",\n",
    "             \"D_fake_loss\",\"D_real_loss\",\"D_cls_loss\",\"G_loss\",\"D_loss\"]\n",
    "metrics_list = []\n",
    "for itemname in lossnames:\n",
    "    metrics_list.append(tf.keras.metrics.Mean(itemname, dtype=tf.float32))\n",
    "\n",
    "for epoch in range(1,200+1):\n",
    "    for (one_Ax, one_Ax_landmark), (one_By, one_By_landmark) in zip(Ax_ds, By_ds):\n",
    "        # train_flownet_step(one_Ax,one_By,one_Ax_landmark,one_By_landmark,tf.cast(epoch,'float32'))\n",
    "        train_items = train_step(one_Ax,one_By,one_Ax_landmark,one_By_landmark,tf.cast(epoch,'float32'))\n",
    "        for (idx, itemname) in enumerate(lossnames):\n",
    "            metrics_list[idx](train_items[itemname])\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    for idx,itemname in enumerate(lossnames):\n",
    "        print(\"    {}: {:.4f}\".format(itemname,metrics_list[idx].result()))\n",
    "    #print(\"epoch: {}, G_loss: {:.4f}, D_loss: {:.4f}, flow_loss: {:.3}\".format(epoch, \n",
    "    #                                                         metrics_list[9].result(),metrics_list[10].result(),\n",
    "    #                                                         metrics_list[0].result()+metrics_list[1].result()))\n",
    "    for metric in metrics_list:\n",
    "        metric.reset_states()\n",
    "    if epoch % 5 == 0:\n",
    "        for (one_Ax, one_Ax_landmark), (one_By, one_By_landmark) in zip(Ax_ds, By_ds):\n",
    "            g_items = generator(one_Ax,one_By,tf.cast(epoch,'float32'),False)\n",
    "            save_images([one_Ax,one_By,g_items['By_warpped'].numpy(),\n",
    "                         g_items['masks'][2][:,:,:,0],\n",
    "                         g_items['masks'][0][:,:,:,0],\n",
    "                         g_items['raw_fake_Ay'].numpy(),g_items['fake_Ay'].numpy(),g_items['residual_Ay'].numpy(),\n",
    "                         g_items['fakeAy_to_Ax'].numpy(),g_items['masks'][1][:,:,:,0],\n",
    "                         g_items['fake_Bx'].numpy(),g_items['fakeBx_to_By'].numpy()],\n",
    "                        epoch,BATCH_SIZE,'GeoGAN1126')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Just for flow net\n",
    "lossnames = [\"G_flow_loss\",\"G_land_loss\"]\n",
    "metrics_list = []\n",
    "for itemname in lossnames:\n",
    "    metrics_list.append(tf.keras.metrics.Mean(itemname, dtype=tf.float32))\n",
    "    \n",
    "for epoch in range(1,400+1):\n",
    "    for (one_Ax, one_Ax_landmark), (one_By, one_By_landmark) in zip(Ax_ds, By_ds):\n",
    "        train_items = train_flowNet_step(one_Ax,one_By,one_Ax_landmark,one_By_landmark,tf.cast(epoch,'float32'))\n",
    "        for (idx, itemname) in enumerate(lossnames):\n",
    "            metrics_list[idx](train_items[itemname])\n",
    "    print(\"epoch: {}, G_flow_loss: {}, G_land_loss: {}\".format(epoch, metrics_list[0].result(),metrics_list[1].result()))\n",
    "    metrics_list[0].reset_states()\n",
    "    metrics_list[1].reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- show temp image(processing)\n",
    "fig = plt.figure(figsize=(75, 75))\n",
    "gs = matplotlib.gridspec.GridSpec(1, 4, width_ratios=[1, 1, 1, 1],\n",
    "        wspace=0.0, hspace=0.0, top=0.95, bottom=0.05, left=0.1, right=0.2)\n",
    "img_idx = 6\n",
    "imgs = [testAx[img_idx],testBy[img_idx],g_items['fakeBx_to_By'][img_idx],g_items['masks'][1][img_idx,:,:,0]]\n",
    "for idx,img in enumerate(imgs):\n",
    "    ax = plt.subplot(gs[0,idx])\n",
    "    ax.axis('off')\n",
    "    ax.imshow(np.clip((img+1)/2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- show temp image(showing)\n",
    "fig = plt.figure(figsize=(75, 75))\n",
    "gs = matplotlib.gridspec.GridSpec(1, 4, width_ratios=[1, 1, 1, 1],\n",
    "        wspace=0.0, hspace=0.0, top=0.95, bottom=0.05, left=0.1, right=0.2)\n",
    "img_idx = 7\n",
    "imgs = [testAx[img_idx],testBy[img_idx],test_By_warpped[img_idx],test_raw_Ay[img_idx]]\n",
    "for idx,img in enumerate(imgs):\n",
    "    ax = plt.subplot(gs[0,idx])\n",
    "    ax.axis('off')\n",
    "    ax.imshow(np.clip((img+1)/2,0,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
